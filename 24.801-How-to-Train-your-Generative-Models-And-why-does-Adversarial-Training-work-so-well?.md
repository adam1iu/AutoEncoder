* [How to Train your Generative Models? And why does Adversarial Training work so well?](http://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/)
    * 当目标是训练一个能够生成自然样本的模型时，极大似然不是一个理想的训练目标。 极大似然是一致的，−Ex∼QlogP(x)
    所以如果给出无限数据和完美模型类，它可以学习任何分布。 然而，在模型错误指定和有限数据下，它都倾向于生成过度概括的模型。
    * KL[P∥Q]可以度量两个分布差距。
    * KL散度是度量用一个分布来近似另一个分布时的信息损失量，不能度量距离。
    因为KL散度不对称 — KL(q||p)≠KL(p||q)— 而距离度量是对称的。
    * 极大似然和 KL[Q∥P] 差别
        * 极大似然和最小化 KL[P∥Q] 是一样的
        * 处理无穷数据的时候是一样的
        * 处理有限数据方式不一样
            * KL[P∥Q]  趋向于Q过度生成p. 如果p是多模型, 优化Q将趋向于覆盖所有p的模型，即使当p=0的时候损失很大。
             实际上，这意味着模型偶尔会采样看起来不像P样本的不合理样本。
            * KL [Q || P]倾向于低于泛化。 最优Q将典型地描述P的单一最大模式，
            如果它们在不覆盖低概率区域的情况下难以建模，代价是忽略其他模式。
            实际上，这意味着KL [Q || P]将尝试避免引入不合理的样本，有时会以P的大部分可疑样本丢失为代价。
        * KL[P∥Q] 自由的, KL[Q∥P]  保守的. 
        * 当然问题在于KL [Q || P]超出了P的有限样本难以优化的程度。甚至比最大似然更难。 
        不仅如此，KL散度的表现也不是很好，而且没有明确定义，除非P在Q为正的任何地方都是正值。 
        所以我们没有希望把KL [Q || P]变成一个实用的训练算法
    * KL[Q || P]是很难优化的，当训练样本的有限的时候,而且除非任何Q为正的地方P也为正的。负责表现不是很好。

99
