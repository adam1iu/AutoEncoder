### 表示能力、层的大小和深度

* 深度自编码器（编码器至少包含一层额外隐藏层）在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。
* 深度可以指数地降低表示某些函数的计算成本。
* 深度也能指数地减少学习一些函数所需的训练数据量。实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩效率。
* 训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经常会遇到浅层自编码器。

* 万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络能以任意精度近似任意函数（在很大范围里），
这是非平凡深度（至少有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数。
但是，从输入到编码的映射是浅层的。这意味这我们不能任意添加约束，比如约束编码稀疏。
* 每次当网络快要拟合数据的时候， 网络学习的进步最大，那么是不是残差网络就是最好的，总是达到最好的拟合。

![](readme/81.000-表示能力、层大小和深度_01.png)
